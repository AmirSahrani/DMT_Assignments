{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import missingno as mo\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy import stats\n",
    "from scipy.stats import shapiro\n",
    "# from OUTLIERS import smirnov_grubbs as grubbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- [ ] Add more visualizations (Sophie)\n",
    "- [x] Missing values          (Amir)\n",
    "  - [x] backfill for 1 day\n",
    "  - [x] In case of long missing values, we still have to figure out what to do \n",
    "    - Most missing values after the day baqckfilling ended up being the most obscure appCat's so I simply set them to 0\n",
    "  - [x] Aggregating over Days/Hours\n",
    "\n",
    "- [ ] Outlier detection        (Sophie)\n",
    "  - [ ] Grubs/ VIF/ Cook's distance (Not all of them)\n",
    "\n",
    "- [ ] Feature engineering     (Nabila)\n",
    "  - [ ] PCA                   \n",
    "    - [ ] Whitening\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.size\": 18,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"axes.labelsize\": 14,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"figure.figsize\": (8, 6),\n",
    "    \"figure.dpi\": 100,\n",
    "    \"savefig.dpi\": 200,\n",
    "    \"savefig.format\": \"png\",\n",
    "    \"savefig.transparent\": True,\n",
    "    \"axes.grid\": True,\n",
    "    \"grid.linewidth\": 0.5,\n",
    "    \"grid.linestyle\": \"--\",\n",
    "    \"grid.color\": \"0.8\",\n",
    "    \"image.cmap\": \"Blues\",\n",
    "    \"lines.linewidth\": 1.5,\n",
    "    \"lines.markersize\": 6,\n",
    "    \"text.usetex\": True, \"mathtext.fontset\": \"cm\",\n",
    "    \"pgf.preamble\": r\"\\usepackage[utf8]{inputenc}\\usepackage[T1]{fontenc}\\usepackage{cmbright}\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/raw/dataset_mood_smartphone.csv\", index_col=0)\n",
    "data['time'] = pd.to_datetime(data['time'])\n",
    "print(data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time column to datetime format\n",
    "data['time'] = pd.to_datetime(data['time'])\n",
    "data['time'] = data['time'].dt.round('H')\n",
    "\n",
    "\n",
    "# Pivot the data to create separate columns for each variable\n",
    "data_pivot = data.pivot_table(index=['id', 'time'], columns='variable', values='value')\n",
    "\n",
    "# Reset the index to flatten the column hierarchy\n",
    "data_pivot.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outliers from visual inspection\n",
    "\n",
    "# Negative value from 'builtin' \n",
    "negative_builtin_indices = np.where(data_pivot['appCat.builtin'] < 0)\n",
    "data_pivot = data_pivot.drop(negative_builtin_indices[0])\n",
    "\n",
    "#Extreme positive value from 'entertainment' \n",
    "positive_ent_indices = np.where(data_pivot['appCat.builtin'] > 30000)\n",
    "data_pivot = data_pivot.drop(positive_ent_indices[0])\n",
    "\n",
    "#Extreme positive value from 'office' \n",
    "positive_office_indices = np.where(data_pivot['appCat.office'] > 30000)\n",
    "data_pivot = data_pivot.drop(positive_office_indices[0])\n",
    "\n",
    "#Extreme positive value from 'social' \n",
    "positive_social_indices = np.where(data_pivot['appCat.social'] > 30000)\n",
    "data_pivot = data_pivot.drop(positive_social_indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_pivot.describe().to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|       | time                          |     activity |   appCat.builtin |   appCat.communication |   appCat.entertainment |   appCat.finance |   appCat.game |   appCat.office |   appCat.other |   appCat.social |   appCat.travel |   appCat.unknown |   appCat.utilities |   appCat.weather |   call |   circumplex.arousal |   circumplex.valence |        mood |        screen |   sms |\n",
    "|:------|:------------------------------|-------------:|-----------------:|-----------------------:|-----------------------:|-----------------:|--------------:|----------------:|---------------:|----------------:|----------------:|-----------------:|-------------------:|-----------------:|-------:|---------------------:|---------------------:|------------:|--------------:|------:|\n",
    "| count | 27028                         | 27026        |      27027       |             27027      |              27026     |       26287      |    23775      |      26974      |     27025      |      27026      |      27022      |       27019      |         27027      |       25009      |  27022 |         27028        |         27028        | 27028       | 27026         | 27005 |\n",
    "| mean  | 2014-04-14 16:34:06.144738816 |     0.120058 |         42.3545  |                45.6104 |                120.543 |          31.3167 |      356.289  |         45.7802 |        20.9953 |         89.0727 |         50.4012 |          42.3354 |            18.4875 |          29.719  |      1 |            -0.217589 |             0.668732 |     6.94341 |   113.027     |     1 |\n",
    "| min   | 2014-02-17 07:00:00           |     0        |      -6343.29    |                 0.117  |                  0.005 |           1.003  |        1.003  |          0.044  |         0.014  |          0.552  |          0.08   |           0.111  |             0.976  |           1.007  |      1 |            -2        |            -2        |     1       |     0.0350001 |     1 |\n",
    "| 25%   | 2014-04-02 06:45:00           |     0        |          4.41667 |                13.12   |                 10.794 |           8.014  |       80.6005 |         10.046  |         8.037  |         17.861  |         22.122  |          11.044  |             4.033  |           5.22   |      1 |            -1        |             0        |     6       |    19.334     |     1 |\n",
    "| 50%   | 2014-04-15 15:00:00           |     0.025    |          8.02163 |                27.6707 |                 34.246 |          21.601  |      169.699  |         23.264  |        10.257  |         44.566  |         38.4282 |          21.185  |            10.537  |          10.147  |      1 |             0        |             1        |     7       |    44.771     |     1 |\n",
    "| 75%   | 2014-04-27 22:00:00           |     0.158333 |         16.3092  |                51.6465 |                123.965 |          51.205  |      478.826  |         45.241  |        16.514  |        102.588  |         61.5086 |          30.178  |            20.0615 |          22.431  |      1 |             1        |             1        |     8       |    94.8592    |     1 |\n",
    "| max   | 2014-06-09 00:00:00           |     1        |      19453       |              3133.48   |               2929.61  |         315.481  |     4115.3    |       4675.7    |      2243.24   |      15001.5    |       3492.32   |        2239.94   |           447.918  |         344.863  |      1 |             2        |             2        |    10       |  9539.82      |     1 |\n",
    "| std   | nan                           |     0.190739 |        582.008   |                85.285  |                217.58  |          29.7814 |      634.744  |         80.0885 |        56.8702 |        161.532  |         76.5206 |         108.084  |            38.2665 |          70.1424 |      0 |             1.0681   |             0.686026 |     1.0813  |   338.403     |     0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the screen time for the entire day, sum the number of sms and call for the entire day\n",
    "data_pivot['day'] = data_pivot['time'].dt.floor('D')\n",
    "data_pivot['screen'] = data_pivot['screen'].groupby([data_pivot['id'], data_pivot['day']]).transform('sum')\n",
    "data_pivot['sms'] = data_pivot['sms'].groupby([data_pivot['id'], data_pivot['day']]).transform('sum')\n",
    "data_pivot['call'] = data_pivot['call'].groupby([data_pivot['id'], data_pivot['day']]).transform('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the number of missing data points\n",
    "missing = data_pivot.isnull().sum()\n",
    "missing = missing[missing > 0]\n",
    "missing.sort_values(inplace=True)\n",
    "missing.plot(kind='bar', title='Number of missing data points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualzing the missing data using a stacked bar plot over time\n",
    "missing = data_pivot.isna().sum(axis=1)\n",
    "missing = missing.groupby(data_pivot['time'].dt.floor('D')).sum()\n",
    "missing.plot(kind='bar', title='Number of missing data points over time', figsize=(15, 5))\n",
    "plt.xticks(np.arange(0, len(missing), 10), missing.index[::10].strftime('%Y-%m-%d'), rotation=45)\n",
    "plt.title(f'Number of missing data points over time, total: {missing.sum()}')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Number of missing data points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the values here there a few missing points, we can use a sliding window to fill in the missing values\n",
    "For the values where there are many value missing, these seem like varaibles where there they have mostly 0 values, so we can fill in the missing values with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data_pivot.columns:\n",
    "    for user in data_pivot['id'].unique():\n",
    "        # window sliding fill\n",
    "        if 0 < data_pivot[column].isnull().sum() < 500:\n",
    "            data_pivot[column][data_pivot['id'] == user] = data_pivot[column][data_pivot['id'] == user].rolling(4, min_periods=1).mean()\n",
    "    \n",
    "\n",
    "missing = data_pivot.isnull().sum()\n",
    "missing = missing[missing > 0]\n",
    "missing.sort_values(inplace=True)\n",
    "missing.plot(kind='bar', title='Number of missing data points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating over days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = data_pivot['time'].dt.floor('D')\n",
    "data_pivot['day'] = day\n",
    "\n",
    "day_data = data_pivot.groupby(['id', 'day']).mean()\n",
    "day_data.reset_index(inplace=True)\n",
    "# Sort data by id and day\n",
    "day_data.sort_values(['id', 'day'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the missing days now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = day_data.isnull().sum()\n",
    "plt.bar(missing.index, missing)\n",
    "plt.xticks(np.arange(0, len(missing), 1), missing.index, rotation=70)\n",
    "plt.show()\n",
    "print(f'Total days measured: {len(day_data)}, days with missing data {len(missing[missing > 0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo.matrix(day_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo.heatmap(day_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo.dendrogram(day_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in day_data.columns:\n",
    "    if 'appCat.' in column:\n",
    "        day_data[column].fillna(0, inplace=True)\n",
    "        \n",
    "day_data.dropna(subset=['mood'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort by id and time, to see how missing values relate between individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo.matrix(day_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo.heatmap(day_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo.dendrogram(day_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a matrix plot per id, for acitivity, circumplex arousal and valence and mood\n",
    "\n",
    "fig, axes = plt.subplots(3,9, figsize=(30, 30))\n",
    "axes = axes.flatten()\n",
    "for i, id in enumerate(day_data['id'].unique()):\n",
    "    id_data = day_data[day_data['id'] == id]\n",
    "    id_data.set_index('time', inplace=True)\n",
    "    id_data = id_data[['activity', 'circumplex.arousal', 'circumplex.valence', 'mood']]\n",
    "    mo.matrix(id_data, ax=axes[i], figsize=(5, 5), sparkline=False)\n",
    "    axes[i].set_title(f'ID: {id}')\n",
    "    # Ensure there is a box around the plot\n",
    "    axes[i].spines['top'].set_visible(True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(day_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows where mood is missing\n",
    "# Make a matrix plot per id, for acitivity, circumplex arousal and valence and mood\n",
    "\n",
    "fig, axes = plt.subplots(3,9, figsize=(30, 30))\n",
    "axes = axes.flatten()\n",
    "for i, id in enumerate(day_data['id'].unique()):\n",
    "    id_data = day_data[day_data['id'] == id]\n",
    "    id_data.set_index('time', inplace=True)\n",
    "    id_data = id_data[['activity', 'circumplex.arousal', 'circumplex.valence', 'mood']]\n",
    "    mo.matrix(id_data, ax=axes[i], figsize=(5, 5), sparkline=False)\n",
    "    axes[i].set_title(f'ID: {id}')\n",
    "    # Ensure there is a box around the plot\n",
    "    axes[i].spines['top'].set_visible(True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo.heatmap(day_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo.dendrogram(day_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['activity', 'circumplex.arousal', 'circumplex.valence']:\n",
    "    print(f\"Processing column: {column}\")\n",
    "    for user in day_data['id'].unique():\n",
    "        user_data = day_data[day_data['id'] == user][column]\n",
    "        \n",
    "        # Perform linear interpolation\n",
    "        interpolated_data = user_data.interpolate(method='linear')\n",
    "        \n",
    "        # Check if there are still NaNs and attempt to fill them\n",
    "        if interpolated_data.isna().any():\n",
    "            # Fill NaNs at the beginning and end by forward and backward filling\n",
    "            interpolated_data.fillna(method='ffill', inplace=True)\n",
    "            interpolated_data.fillna(method='bfill', inplace=True)\n",
    "\n",
    "        # Assign back the interpolated (and potentially forward/backward filled) data\n",
    "        day_data.loc[day_data['id'] == user, column] = interpolated_data\n",
    "\n",
    "print(day_data.isnull().sum())\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(day_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "We can see that generally all data for each participant is complete, but with chucks missing, this might mean we can simply just drop the missing chunks of data, but this means that we can't predict someones mood for the first few days when they enter. It might be interesting to see if we can predict the mood even for these days using the other people's data and the data we have on the other stuff for that person, but it would require a special encoding of the data to differentiate between missing data and actual data. E.g. a value outside of the range of valid numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a toy data set to test model training\n",
    "\n",
    "# Create a toy data set with 10 days of data for 10 users\n",
    "np.random.seed(42)\n",
    "users = np.arange(10)\n",
    "days = pd.date_range('2014-02-01', periods=10, freq='D')\n",
    "toy_data = pd.DataFrame(index=pd.MultiIndex.from_product([users, days], names=['id', 'time']))\n",
    "# Give each user a random mean and std, use that to generate random data\n",
    "for user in users:\n",
    "    mean = np.random.uniform(0, 10)\n",
    "    std = np.random.uniform(0, 5)\n",
    "    for column in day_data.columns:\n",
    "        if column in ['id', 'day', 'time']:\n",
    "            continue\n",
    "        else:\n",
    "            toy_data.loc[user, column] = np.random.exponential(mean, size=10) / (day_data[column].max() - day_data[column].min()) + day_data[column].min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data.reset_index(inplace=True)\n",
    "toy_data.to_csv('../data/preprocessed/toy_data.csv', index=False)\n",
    "toy_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get correlation matrix ignoring time and id\n",
    "\n",
    "# numeric only\n",
    "num_grouped = data_pivot.select_dtypes(include=[np.number])\n",
    "# Drop call and sms variables\n",
    "num_grouped = num_grouped.drop(columns=['call', 'sms'])\n",
    "corr = num_grouped.corr()\n",
    "plt.imshow(corr, cmap='Blues', interpolation='nearest')\n",
    "plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
    "plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Data Visualizations, i.e. stuff Sophie just added\n",
    "It seems like something might be strange about the call and SMS data because they produce the exact same graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histograms for Call and SMS overall\n",
    "\n",
    "sms_data = data_pivot[\"sms\"]\n",
    "call_data = data_pivot[\"call\"]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "axes[0].hist(sms_data, bins=30, color='blue', alpha=0.5)\n",
    "axes[0].set_xlabel(\"Number of SMSs Sent\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].set_title('SMS')\n",
    "\n",
    "axes[1].hist(call_data, bins=30, color='green', alpha=0.5)\n",
    "axes[1].set_xlabel(\"Number of Calls Made\")\n",
    "axes[1].set_title('Call')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar plots for call and SMS over time\n",
    "\n",
    "days = data_pivot['day']\n",
    "sms_counts = data_pivot['sms']\n",
    "call_counts = data_pivot['call']\n",
    "time_sorted = sorted(days)\n",
    "sorted_sms_counts = [sms_counts[index] for index in sorted(range(len(days)), key=lambda x: days[x])]\n",
    "sorted_call_counts = [call_counts[index] for index in sorted(range(len(days)), key=lambda x: days[x])]\n",
    "months = [date.strftime('%b') for date in time_sorted]\n",
    "\n",
    "#SMS Data\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "axes[0].bar(months, sorted_sms_counts, color='blue', alpha=0.5)\n",
    "axes[0].set_ylabel('SMS Count')\n",
    "axes[0].set_title('SMS Count Over Time')\n",
    "\n",
    "#Call Data\n",
    "axes[1].bar(months, sorted_call_counts, color='green', alpha=0.5)\n",
    "axes[1].set_ylabel('Call Count')\n",
    "axes[1].set_title('Call Count Over Time')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Identification\n",
    "The only non-normally distributed variable for the unbounded data is \"screen\". Grubb's tests for the normally distributed unbounded data all return none. Previous data cleaning must remove them.\n",
    "\n",
    "Other methods, such as IQR and Z-score for identifying outliers in screen indicate that the outliers for screen are likely just 'natural outliers' and don't need to be removed from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shapiro-Wilk Tests for normality to see if we can use Grubb's Test\n",
    "\n",
    "variables = ['screen', 'appCat.builtin', 'appCat.communication', 'appCat.entertainment', 'appCat.finance', \n",
    "             'appCat.game', 'appCat.office', 'appCat.other', 'appCat.social', 'appCat.travel', 'appCat.unknown', \n",
    "             'appCat.utilities']\n",
    "\n",
    "normality_results = {}\n",
    "\n",
    "for variable in variables:\n",
    "    data = data_pivot[variable]\n",
    "    stat, p_value = shapiro(data)\n",
    "    if p_value > 0.05:\n",
    "        normality_results[variable] = 'Normally distributed'\n",
    "    else:\n",
    "        normality_results[variable] = 'Not normally distributed'\n",
    "\n",
    "for variable, result in normality_results.items():\n",
    "    print(f\"Result for {variable}: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying some methods for identifying outliers. First, with the most basic using interquartile range.\n",
    "\n",
    "def identify_outliers_IQR(data):\n",
    "    Q1 = np.percentile(data, 25)\n",
    "    Q3 = np.percentile(data, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    upper_outliers = np.where(data >= upper_bound)[0]\n",
    "    lower_outliers = np.where(data <= lower_bound)[0]\n",
    "    return lower_outliers, upper_outliers\n",
    "\n",
    "IQR_outliers_info = {}\n",
    "for variable in variables: \n",
    "    IQR_outliers_info[variable] = identify_outliers_IQR(data_pivot[variable])\n",
    "    #print(f\"Outliers in {variable} found at indices: {outliers_info.get}\")\n",
    "\n",
    "for variable, result in IQR_outliers_info.items():\n",
    "    if len(result[0] > 0) or len(result[1] > 0):\n",
    "        print(f'Lower outliers in {variable} found at indices {result[0]}.')\n",
    "        print(f'Upper outliers in {variable} found at indices {result[1]}.')\n",
    "        print()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying some methods for identifying outliers. First, with the most basic using interquartile range.\n",
    "\n",
    "def identify_outliers_zScore(data):\n",
    "    z = np.abs(stats.zscore(data))\n",
    "    threshold_z = 2\n",
    "    outlier_indices = np.where(z > threshold_z)[0]\n",
    "    return outlier_indices\n",
    "\n",
    "zScore_outliers_info = {}\n",
    "\n",
    "for variable in variables: \n",
    "    zScore_outliers_info[variable] = identify_outliers_zScore(data_pivot[variable])\n",
    "\n",
    "for variable, result in zScore_outliers_info.items():\n",
    "    if len(result) > 0:\n",
    "        print(f'Outliers in {variable} found at indices {result}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Outliers\n",
    "data_pivot = data_pivot.sort_values(by='time')\n",
    "IQR_indices = np.concatenate((IQR_outliers_info['screen'][0],IQR_outliers_info['screen'][1]))\n",
    "zScore_indices = zScore_outliers_info['screen']\n",
    "common_indices = np.intersect1d(IQR_indices, zScore_indices)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(data_pivot['time'], data_pivot['screen'], color = 'blue')\n",
    "plt.scatter(data_pivot['time'][IQR_indices], data_pivot['screen'][IQR_indices], color='red')\n",
    "#plt.scatter(data_pivot['time'][zScore_indices], data_pivot['screen'][zScore_indices], color='yellow')\n",
    "plt.scatter(data_pivot['time'][common_indices], data_pivot['screen'][common_indices], color='orange')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Screen Time\")\n",
    "plt.title(\"Outliers in 'Screen'\")\n",
    "plt.legend([\"Non-outlier points\", \"IQR Outliers\",\"Both IQR and Z-Score Outliers\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grubb's Test for normally distributed data\n",
    "\n",
    "variables = ['appCat.builtin', 'appCat.communication', 'appCat.entertainment', 'appCat.finance', \n",
    "             'appCat.game', 'appCat.office', 'appCat.other', 'appCat.social', 'appCat.travel', 'appCat.unknown', \n",
    "             'appCat.utilities']\n",
    "\n",
    "grubbs_results = {}\n",
    "for variable in variables:\n",
    "    outlier_index = grubbs.max_test_indices(data_pivot[variable], alpha=.05)\n",
    "    grubbs_results[variable] = outlier_index\n",
    "\n",
    "for variable, result in grubbs_results.items():\n",
    "    print(f\"Result for {variable}: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ComputationalEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
